{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c3f9fa-6b1e-41d4-a0e8-0f98ccd84172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632bf58d-bee4-4abe-b242-ccd729c42df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 8)\n"
     ]
    }
   ],
   "source": [
    "data_T0 = pd.read_csv(\"QueryResults.csv\")\n",
    "print(data_T0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6bd5bb-b52b-4a26-bf37-995845140477",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.56 GiB for an array with shape (30000, 11434) and data type uint64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m clean_tags \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m><\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: [a\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Use pandas' get_dummies to get dummy values \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# select only tags that appear over 500 times\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tag_columns \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m all_tags \u001b[38;5;241m=\u001b[39m tag_columns\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m top_tags \u001b[38;5;241m=\u001b[39m all_tags[all_tags \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m50000\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:3146\u001b[0m, in \u001b[0;36mGroupBy.sum\u001b[1;34m(self, numeric_only, min_count, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   3141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3142\u001b[0m     \u001b[38;5;66;03m# If we are grouping on categoricals we want unobserved categories to\u001b[39;00m\n\u001b[0;32m   3143\u001b[0m     \u001b[38;5;66;03m# return zero, rather than the default of NaN which the reindexing in\u001b[39;00m\n\u001b[0;32m   3144\u001b[0m     \u001b[38;5;66;03m# _agg_general() returns. GH #31422\u001b[39;00m\n\u001b[0;32m   3145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 3146\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3149\u001b[0m \u001b[43m            \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3150\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnpfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3151\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_output(result, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1906\u001b[0m, in \u001b[0;36mGroupBy._agg_general\u001b[1;34m(self, numeric_only, min_count, alias, npfunc, **kwargs)\u001b[0m\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_agg_general\u001b[39m(\n\u001b[0;32m   1898\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1904\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1905\u001b[0m ):\n\u001b[1;32m-> 1906\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[0;32m   1907\u001b[0m         how\u001b[38;5;241m=\u001b[39malias,\n\u001b[0;32m   1908\u001b[0m         alt\u001b[38;5;241m=\u001b[39mnpfunc,\n\u001b[0;32m   1909\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[0;32m   1910\u001b[0m         min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m   1911\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1912\u001b[0m     )\n\u001b[0;32m   1913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[0;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1472\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1470\u001b[0m             result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m   1471\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1472\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_blocks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1973\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_func\u001b[39m(values: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[0;32m   1972\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1973\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39m_cython_operation(\n\u001b[0;32m   1974\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1975\u001b[0m             values,\n\u001b[0;32m   1976\u001b[0m             how,\n\u001b[0;32m   1977\u001b[0m             axis\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1978\u001b[0m             min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m   1979\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1980\u001b[0m         )\n\u001b[0;32m   1981\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m   1982\u001b[0m         \u001b[38;5;66;03m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;66;03m# and non-applicable functions\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m         \u001b[38;5;66;03m# try to python agg\u001b[39;00m\n\u001b[0;32m   1985\u001b[0m         \u001b[38;5;66;03m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[0;32m   1986\u001b[0m         \u001b[38;5;66;03m# TODO: avoid special casing SparseArray here\u001b[39;00m\n\u001b[0;32m   1987\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, SparseArray):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:831\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m ids, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_info\n\u001b[0;32m    830\u001b[0m ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups\n\u001b[1;32m--> 831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cy_op\u001b[38;5;241m.\u001b[39mcython_operation(\n\u001b[0;32m    832\u001b[0m     values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[0;32m    833\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    834\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m    835\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    836\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:550\u001b[0m, in \u001b[0;36mWrappedCythonOp.cython_operation\u001b[1;34m(self, values, axis, min_count, comp_ids, ngroups, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39m_groupby_op(\n\u001b[0;32m    542\u001b[0m         how\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow,\n\u001b[0;32m    543\u001b[0m         has_dropped_na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_dropped_na,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_op_ndim_compat(\n\u001b[0;32m    551\u001b[0m     values,\n\u001b[0;32m    552\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m    553\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[0;32m    554\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mcomp_ids,\n\u001b[0;32m    555\u001b[0m     mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    557\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:344\u001b[0m, in \u001b[0;36mWrappedCythonOp._cython_op_ndim_compat\u001b[1;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;66;03m# otherwise we have OHLC\u001b[39;00m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_cython_op(\n\u001b[0;32m    345\u001b[0m     values,\n\u001b[0;32m    346\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m    347\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[0;32m    348\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mcomp_ids,\n\u001b[0;32m    349\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    350\u001b[0m     result_mask\u001b[38;5;241m=\u001b[39mresult_mask,\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    352\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:418\u001b[0m, in \u001b[0;36mWrappedCythonOp._call_cython_op\u001b[1;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(ngroups, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    417\u001b[0m ]:\n\u001b[1;32m--> 418\u001b[0m     func(\n\u001b[0;32m    419\u001b[0m         out\u001b[38;5;241m=\u001b[39mresult,\n\u001b[0;32m    420\u001b[0m         counts\u001b[38;5;241m=\u001b[39mcounts,\n\u001b[0;32m    421\u001b[0m         values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[0;32m    422\u001b[0m         labels\u001b[38;5;241m=\u001b[39mcomp_ids,\n\u001b[0;32m    423\u001b[0m         min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m    424\u001b[0m         mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    425\u001b[0m         result_mask\u001b[38;5;241m=\u001b[39mresult_mask,\n\u001b[0;32m    426\u001b[0m         is_datetimelike\u001b[38;5;241m=\u001b[39mis_datetimelike,\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    428\u001b[0m     )\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mohlc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msem\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mgroupby.pyx:694\u001b[0m, in \u001b[0;36mpandas._libs.groupby.group_sum\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.56 GiB for an array with shape (30000, 11434) and data type uint64"
     ]
    }
   ],
   "source": [
    "# Select our tags, represented as strings, and transform them into arrays of tags\n",
    "tags = data_T0[\"Tags\"]\n",
    "clean_tags = tags.str.split(\"><\").apply(\n",
    "    lambda x: [a.strip(\"<\").strip(\">\") for a in x])\n",
    "\n",
    "# Use pandas' get_dummies to get dummy values \n",
    "# select only tags that appear over 500 times\n",
    "tag_columns = pd.get_dummies(clean_tags.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "all_tags = tag_columns.copy().astype(bool).sum(axis=0).sort_values(ascending=False)\n",
    "top_tags = all_tags[all_tags > 50000//100]\n",
    "top_tag_columns = tag_columns[top_tags.index]\n",
    "\n",
    "data_T1 = pd.concat([data_T0, top_tag_columns], axis=1)\n",
    "data_T1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974ecd6-1791-4909-ad8b-a952b11dfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_T1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c79697-dfab-42cf-a360-055e4fcbbc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def bracket_exterminator(sentence):\n",
    "    ret = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in sentence:\n",
    "        if i == '<':\n",
    "            skip1c += 1\n",
    "        elif i == '>' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            ret += i\n",
    "    return ret\n",
    "\n",
    "def code_exterminator(sentence):\n",
    "    idx1 = sentence.find(\"<code>\")\n",
    "    idx2 = sentence.find(\"<\\code>\")\n",
    "    res = sentence[:idx1] +  sentence[:idx2 + len(\"<\\code>\") + 1]\n",
    "    return res\n",
    "\n",
    "def tokenizer_fct(sentence) :\n",
    "    # print(sentence)\n",
    "    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').replace('#', ' ').replace('. ', ' ').replace('=', ' ').replace('(', ' ').replace(')', ' ').replace('{', ' ').replace('}', ' ').replace(',', ' ').replace(';', ' ').replace('.', ' ')\n",
    "    word_tokens = word_tokenize(sentence_clean)\n",
    "    return word_tokens\n",
    "\n",
    "# Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_w = list(set(stopwords.words('english'))) + ['[', ']', ',', ':', '?', '(', ')','{','}']\n",
    "\n",
    "def stop_word_filter_fct(list_words) :\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w2\n",
    "\n",
    "# lower case et alpha\n",
    "def lower_start_fct(list_words) :\n",
    "    lw = [w.lower() for w in list_words if (not w.startswith(\"@\")) \n",
    "    #                                   and (not w.startswith(\"#\"))\n",
    "                                       and (not w.startswith(\"http\"))]\n",
    "    return lw\n",
    "\n",
    "# Lemmatizer (base d'un mot)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_fct(list_words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "\n",
    "# Fonction de préparation du texte pour le bag of words avec lemmatization\n",
    "def transform_bow_lem_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)    \n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "data_T1['Body'] = data_T1['Body'].apply(lambda x : code_exterminator(x))\n",
    "data_T1['Text'] = data_T1['Title'] + \" \" + data_T1['Body']\n",
    "data_T1['Text'] = data_T1['Text'].apply(lambda x : bracket_exterminator(x))\n",
    "data_T1['sentence_bow_lem'] = data_T1['Text'].apply(lambda x : transform_bow_lem_fct(x))\n",
    "data_T1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0190b91-346a-421c-b01c-17367089215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Calcul Tsne, détermination des clusters et calcul ARI entre vrais catégorie et n° de clusters\n",
    "def ARI_fct(features) :\n",
    "    time1 = time.time()\n",
    "    num_labels=len(l_cat)\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=50, max_iter=2000, \n",
    "                                 init='random', learning_rate=100, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    \n",
    "    # Détermination des clusters à partir des données après Tsne \n",
    "    cls = cluster.KMeans(n_clusters=num_labels, n_init=100, random_state=42)\n",
    "    cls.fit(X_tsne)\n",
    "    ARI = np.round(metrics.adjusted_rand_score(y_cat_num, cls.labels_),4)\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"ARI : \", ARI, \"time : \", time2)\n",
    "    \n",
    "    return ARI, X_tsne, cls.labels_\n",
    "\n",
    "\n",
    "# visualisation du Tsne selon les vraies catégories et selon les clusters\n",
    "def TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI) :\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    \n",
    "    ax = fig.add_subplot(121)\n",
    "    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=y_cat_num, cmap='Set1')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=l_cat, loc=\"best\", title=\"Categorie\")\n",
    "    plt.title('Représentation des tweets par catégories réelles')\n",
    "    \n",
    "    ax = fig.add_subplot(122)\n",
    "    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=labels, cmap='Set1')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=set(labels), loc=\"best\", title=\"Clusters\")\n",
    "    plt.title('Représentation des tweets par clusters')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"ARI : \", ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21a9b5-05ef-457a-be5e-34d1de70c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "\n",
    "cvect = CountVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "ctf = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "\n",
    "feat = 'sentence_bow_lem'\n",
    "cv_fit = cvect.fit(data_T1[feat])\n",
    "ctf_fit = ctf.fit(data_T1[feat])\n",
    "\n",
    "cv_transform = cvect.transform(data_T1[feat])  \n",
    "ctf_transform = ctf.transform(data_T1[feat])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea20296-e265-4d68-baed-cbeff2850f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, perplexity=50, max_iter=2000, init='random', learning_rate=100, random_state=42)\n",
    "X_tsne = tsne.fit_transform(cv_transform)\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "    \n",
    "ax = fig.add_subplot(121)\n",
    "scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], cmap='Set1')\n",
    "ax.legend(handles=scatter.legend_elements()[0], loc=\"best\", title=\"Categorie\")\n",
    "plt.title('Représentation des tweets par catégories réelles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f44a1-8778-4304-b15a-c225d48cde79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"CountVectorizer : \")\n",
    "print(\"-----------------\")\n",
    "ARI, X_tsne, labels = ARI_fct(cv_transform)\n",
    "print()\n",
    "print(\"Tf-idf : \")\n",
    "print(\"--------\")\n",
    "ARI, X_tsne, labels = ARI_fct(ctf_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c85b82-a5cb-4f67-b8a1-a4746ad16c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    " TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d4639-423f-4ff5-b22e-9511bbdbe6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
